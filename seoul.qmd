---
title: "Seasonality in Seoul: Multilinear Modeling with Kth Differences and Fixed Effects"
author: "Ketherine Wang, Sam Lee"
format: pdf
editor: visual
header-includes:
  - \usepackage{amsmath}
---

```{=html}
<style type="text/css">
h1.title {
font-size: 40px;
text-align: center;
}
h4.author {
font-size: 40px;
text-align: center;
}
</style>
```
\newpage

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(GGally)
library(corrplot)
library(patchwork)
library(car)
library(ggfortify)
library(alr4)
library(glmnet)
library(broom)
library(robustbase)

set.seed(12022023)

seoul <- read.csv("SeoulBikeData.csv", header=F, skip=1) %>% setNames(
  c("Date", "RentedBikeCount", "Hour", "Temperature", "Humidity", "WindSpeed",
    "Visibility", "DewPointTemp", "SolarRadiation", "Rainfall", "Snowfall",
    "Seasons", "Holiday", "FunctioningDay")
) %>% as_tibble() %>%
  mutate(
    Date = dmy(Date),
    Time = ymd_h(paste(Date, Hour)),
    IsAutumn = 1*(Seasons == "Autumn"),
    IsSummer = 1*(Seasons == "Summer"),
    IsWinter = 1*(Seasons == "Winter"),
    IsSpring = 1*(Seasons == "Spring"),
    IsHoliday = 1*(Holiday == "Holiday"),
    Hour09 = 1*(Hour %in% 0:9),
    Hour1018 = 1*(Hour %in% 10:18),
    Hour1924 = 1*(Hour %in% 19:24),
    RentedBikeCount = log(RentedBikeCount+1)
  ) %>% arrange(Time)
```

# Abstract

# 1 Problem and Motivation

## 1.1 Data Description

In this analysis we used data from UC Irvine's Machine Learning's Repository. We accessed [Seoul Bike Sharing Data](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand), which contains 8,760 rows of hourly data pertaining to the corresponding amount of rented bikes (bike demand) in Seoul, Korea. 8,465 rows are considered "functioning days," where bikes are able to be rented. In order to consider our problem of interest, we will only consider these 8,465 rows to model the bike demand in Seoul. This data set also contains corresponding data for the respective temperature (celcius), humidity (%), wind speed (m/s), visibility (10m), dew point temperature (celcius), solar radiation (MJ/$m^2$), rainfall precipitation (mm), snowfall precipitation (cm), seasons (Winter, Spring, Summer, Autumn), and an indicator determining whether the corresponding day is a holiday.

We will use these variables along with a subset of interaction terms which will be determined by elastic net regression to include in our models.

## 1.2 Questions of Interest

1.  Can we use factors in weather and time to determine the best predictors of short-run for fluctuations in temperature for the climate in Seoul, Korea? 

2.  Can we determine which weather and time factors affect the bike sharing demand the most in Seoul, Korea?

If there are significant predictive factors, which one has the largest influence?

```{r echo=FALSE}
difference = 5
seasons = unique(seoul$Seasons)
Temp = seoul[c("Time", "Temperature", "Seasons")]

season.base = "IsSpring"
season.dummies = setdiff(c("IsWinter", "IsSpring", "IsSummer", "IsAutumn"), season.base)

time.base = "Hour1924"
time.dummies = setdiff(c("Hour09", "Hour1018"), time.base)

time.bike = c()
for(h in unique(seoul$Hour)){
  factor = str_c("Hour", h)
  seoul[factor] = as.numeric(seoul$Hour == h)
  time.bike = c(time.bike, factor)
}
time.bike.base = "Hour0"
time.bike = setdiff(time.bike, time.bike.base)

time.dummies = time.bike

seoul.factors = c("Temperature", "Humidity", "DewPointTemp",
                  "WindSpeed", "Visibility", "SolarRadiation", "Rainfall",
                  "Snowfall", time.dummies)

seoul.change = c()

for(factor in seoul.factors){
  change_factor = str_c(factor, "Change")
  seoul.change = c(seoul.change, change_factor)
  seoul[change_factor] = NA_real_
  for(s in seasons){
    seoul.subset = seoul %>% filter(Seasons == s)
    seoul[seoul$Seasons == s, change_factor] = seoul.subset[factor]-lag(seoul.subset[factor], n=difference)
  }
}

seoul.interaction = c()
seoul.base = c(time.base)

#All 2d Interaction Effects
continuous_interactions = combn(setdiff(seoul.change, 
                                        c(str_c(time.dummies, "Change"),
                                          str_c(season.dummies, "Change"),
                                          "TemperatureChange")), 2)
for(i in 1:ncol(continuous_interactions)){
  factor = paste(continuous_interactions[,i], collapse = "X")
  seoul.interaction = c(seoul.interaction, factor)
  seoul[factor] = NA_real_
  for(s in seasons){
    seoul.subset = seoul %>% filter(Seasons == s)
    seoul[seoul$Seasons == s, factor] = seoul.subset[continuous_interactions[1,i]]*
                                        seoul.subset[continuous_interactions[2,i]]
  }
}

#Time Interaction Effects
for(factor in setdiff(seoul.change, 
                      c(str_c(time.dummies, "Change"),
                        "TemperatureChange"))){
  for(time in str_c(time.dummies, "Change")){
    factor.interaction = str_c(factor, "X", time)
    seoul.interaction = c(seoul.interaction, factor.interaction)
    seoul[factor.interaction] = NA_real_
    for(s in seasons){
      seoul.subset = seoul %>% filter(Seasons == s)
      seoul[seoul$Seasons == s, factor.interaction] = seoul.subset[factor]*
                                                      seoul.subset[time]
    }
  }
}

###### Rented Bike Count Model

a = 1 #1 is excluded for base dummy
time.humidities = str_c("Humidity",setdiff(1:10, a))
for(h in 1:length(time.humidities)){
  h.min = (h-1+a)*10
  h.max = (h+a)*10
  seoul[time.humidities[h]] = 1*(seoul$Humidity >= h.min &
                                 seoul$Humidity < h.max)
}

bike.dummies = c("IsHoliday")
seoul.bike.factors = c("RentedBikeCount", time.humidities, "Temperature",
                       "WindSpeed", "Visibility", "DewPointTemp",
                       "SolarRadiation", "ExtremeWind",
                       "Rainfall", "Snowfall", bike.dummies, time.bike,
                       season.dummies)

seoul$ExtremeWind = 1*(seoul$WindSpeed > quantile(seoul$WindSpeed, 0.95))

#Transform Visibility
seoul$Visibility = -sqrt(seoul$Visibility)

seoul.bike.interaction = c()
seoul.bike.base = c(time.bike.base)


continuous_interactions_bike = combn(setdiff(seoul.bike.factors, 
                                        c(time.bike, season.dummies,
                                          time.humidities,
                                          "RentedBikeCount")), 2)
for(i in 1:ncol(continuous_interactions_bike)){
  factor = paste(continuous_interactions_bike[,i], collapse = "X")
  seoul.bike.interaction = c(seoul.bike.interaction, factor)
  seoul[factor] = seoul[continuous_interactions_bike[1,i]]*
                  seoul[continuous_interactions_bike[2,i]]
}

for(factor in setdiff(seoul.bike.factors,
                        c(time.bike, season.dummies, "RentedBikeCount"))){
  for(time in time.bike){
    factor.interaction = str_c(factor, "X", time)
    seoul.bike.interaction = c(seoul.bike.interaction, factor.interaction)
    seoul[factor.interaction] = seoul[factor]*seoul[time]
  }
}

#Seasonal Interactions
for(factor in setdiff(seoul.bike.factors,
                        c(season.dummies, "RentedBikeCount"))){
  for(s in season.dummies){
    factor.interaction = str_c(factor, "X", s)
    seoul.bike.interaction = c(seoul.bike.interaction, factor.interaction)
    seoul[factor.interaction] = seoul[factor]*seoul[s]
  }
}

#Humidity and Other Weather Effects
for(factor in setdiff(seoul.bike.factors,
                        c(time.bike, season.dummies,
                                          time.humidities,
                                          "RentedBikeCount"))){
  for(humidity in time.humidities){
    factor.interaction = str_c(factor, "X", humidity)
    seoul.bike.interaction = c(seoul.bike.interaction, factor.interaction)
    seoul[factor.interaction] = seoul[factor]*seoul[humidity]
  }
}

seoul[complete.cases(seoul[seoul.change]), ] -> seoul.temp
```

## 1.3 Regression Methods

Temperature is highly dependent on the temperature from the hour before ($t-1$): $\frac{\hat{\text{Cov}}(\text{Temperature}_t,\text{Temperature}_{t-1})}{\hat{Var}(\text{Temperature}_t)}$ = `r round(lm(seoul$Temperature ~ lag(seoul$Temperature, n=1))$coef[2], 4)` (which is significant). Since we have hourly data, running a first difference regression model will, though eliminate the dependency on the value from $t-1$, may also leave the dependency from $t-2,...t-(k-1)$ left over. If $\Delta\text{Temperature}_t = \text{Temperature}_t - \text{Temperature}_{t-k}$, We will find the $k$ (the $k$th difference) such that there is no longer a direct dependency between $\Delta\text{Temperature}_t$ and $\Delta\text{Temperature}_{t-k}$. In other words, We want to find the smallest possible $k$ such that $cov(\Delta\text{Temperature}_t, \Delta\text{Temperature}_{t-k}) = 0$.

We found that the optimal choice of $k$ was $5$ (see [A.0](#a0) for optimal selection process on $k$).

We also hypothesized that since there are seasonal effects that are unobservable in our data set, the error term, $\epsilon$ is composed of a season-specific error term, $\alpha_{s}$, and everything else that may affect fluctuations in temperature that varies by time period ($t$) and the season, which aren't possible to explicitly include in the model. Hence, $\epsilon_{st} = \alpha_{s} + \eta_{st}$. Let $s$ denote the season and $t$ denote time. For our data set-specific purposes, we will seek to understand how $\text{Temperature}_{st}$ changes with respect to each season and hour ($t$).

Hence, with highly dependent time series data, we will resort to using a $k$th-differences regression model to model the short-run fluctuation in temperature where $k = 5$. In other words, we are going to try to see how temperature changes every five hours and see which factors affect those changes the most.

Hence, for our basic multivariate regression model (w/o interaction terms), if 

$\text{Temperature}_{st} = \beta_0 + \beta_1\text{Humidity}_{st} + \beta_2\text{DewPointTemp}_{st} + \beta_3\text{WindSpeed}_{st} + \beta_4\text{SolarRadiation}_{st} + \beta_5\text{Rainfall}_{st} + \beta_6\text{Snowfall}_{st} + \beta_7I(\text{Hour}_t = 1) + ... + \beta_{29}I(\text{Hour}_t = 23) + \alpha_{s} + \eta_{st}$

Then,

$\text{Temperature}_{st-5} = \beta_0 + \beta_1\text{Humidity}_{st-5} + \beta_2\text{DewPointTemp}_{st-5} + \beta_3\text{WindSpeed}_{st-5} + \beta_4\text{SolarRadiation}_{st-5} + \beta_5\text{Rainfall}_{st-5} + \beta_6\text{Snowfall}_{st-5} + \beta_7I(\text{Hour}_{t-5} = 1) + ... + \beta_{29}I(\text{Hour}_{t-5} = 23) + \alpha_{s} + \eta_{st-5}$

Thus, we wish to estimate,

(1) $\Delta\text{Temperature}_{st} = \beta_1\Delta\text{Humidity}_{st} + \beta_2\Delta\text{DewPointTemp}_{st} + \beta_3\Delta\text{WindSpeed}_{st} + \beta_4\Delta\text{SolarRadiation}_{st} + \beta_5\Delta\text{Rainfall}_{st} + \beta_6\Delta\text{Snowfall}_{st} + \Delta\beta_7I(\text{Hour}_{t} = 1) + ... + \Delta\beta_{29}I(\text{Hour}_{t} = 23) + \Delta\eta_{st}$

Where $\Delta\text{Temperature}_{st} = \text{Temperature}_{st} - \text{Temperature}_{t-5}$

```{r message=F, echo=F, fig.width=16}
corrplot(cor(seoul.temp[seoul.change %>% 
         setdiff(str_c(time.dummies, "Change"))]), 
         type="upper")
```

With our basic multilinear model (1) the correlation matrix indicates that there may be some potential multicolinearity between $\text{DewPointTempChange}_{st}$ and $\text{HumidityChange}_{st}$. For the most part, the indicators look good with changes in $\text{SolarRadiation}_{st}$ potentially affecting flucutations in temperature the most.

```{r echo=F}
seoul.temp %>% ggplot()+
  geom_point(aes(x=Hour, y=TemperatureChange), size=0.25)+
  facet_grid(~Seasons)
```

According to our model, changes in temperature not only depend on temperature but also on season. While there are some general linear trends across some ranges of hours, the non-linearity of the hours in general with respect to temperature change led us to include indicators for each hour as specified in Model (1) using Hour 0 (12:00 a.m.) as our base year.

```{r echo=F}
show_leverage = function(model){
  cd_cont_pos <- function(leverage, level, model) {
    sqrt(level * length(coef(model)) * (1 - leverage) / leverage)
  }
  cd_cont_neg <- function(leverage, level, model) {
    -cd_cont_pos(leverage, level, model)
  }
  
  cd_threshold <- 0.5
  cd_threshold2 <- 4 / (nrow(seoul) - 2)
  x_max_val <- 0.35
  
  autoplot(model, which = 5, nrow = 1, ncol = 1) +
    theme(aspect.ratio = 1) +
    stat_function(fun = cd_cont_pos,
                  args = list(level = cd_threshold, model = model),
                  xlim = c(0, x_max_val), lty = 2, colour = "#FFACEE") +
    stat_function(fun = cd_cont_neg,
                  args = list(level = cd_threshold, model = model),
                  xlim = c(0, x_max_val), lty = 2, colour = "#FFACEE") +
    stat_function(fun = cd_cont_pos,
                  args = list(level = cd_threshold2, model = model),
                  xlim = c(0, x_max_val), lty = 2, colour = "#AFDFFF") +
    stat_function(fun = cd_cont_neg,
                  args = list(level = cd_threshold2, model = model),
                  xlim = c(0, x_max_val), lty = 2, colour = "#AFDFFF")
}
```

According to the model diagnostics [A.1.0](#a10), the linearity assumption seems to met (see [A.1](#a11) for added variable plots). Additionally, though there were some points moving the model away from homoskedasticity the $\sqrt{|\text{standardized residuals}|)}$ vs. Fitted values Plot shows that homoskedasticity is roughly met. However, the normality assumption was blatantly violated so we considered adding in interaction terms.

We used elastic net regression to select interaction terms to see if the influential points (and thus, Normality) could be remedied. We used elastic net regression over lasso regression due to the potential colinearity between $\text{HumidityChange}_{st}$ and other weather effects.

```{r echo=FALSE}
model.factors = setdiff(c(seoul.change, seoul.interaction), 
                        c(seoul.base, "TemperatureChange"))

seoul.x = as.matrix(seoul.temp[model.factors])
seoul.y = as.matrix(seoul.temp["TemperatureChange"])

elastic_cv <- cv.glmnet(x = seoul.x,
                      y = seoul.y, 
                      type.measure = "mse", 
                      alpha = 0.5)

d = coef(elastic_cv, s = "lambda.1se")[2:(length(model.factors)-1)]
elastic.factors = c(model.factors[which(d != 0)])
elastic.factors = c(elastic.factors, setdiff(str_c(time.bike, "Change"), elastic.factors))
seoul.elastic = lm(TemperatureChange ~ 0 + .,
                                       seoul.temp[c("TemperatureChange",
                                       elastic.factors)])
```

(The summary output is too long to be included here, see [A.2.0](#a20) for model output)

However, even with selected interaction terms included, Normality did not drastically improve (see [A.2.1](#a21) for QQ-Plot).

We considered transforming the model ([A.3](#a3)) to better meet Normality and somewhat homoskedasticity. However, due to the nature of our $X$ variable matrix--since each independent variable is a change in a variable, we thus had to shift the entire vector of that corresponding variable in order to derive optimal transformations. Hence, interpreting the any transformations we would make would prove difficult and impractical.

However, with large sample size ($n=8740$), Normality could be appealed to. The residuals do appear to be Normal, though extreme values cause deviations from Normality at the tails as seen in the QQ-plot. We considered alternatives to meeting the assumptions such as linear robust regression to reduce the weights of outliers. One alternative is further discussed in [A.4](#a4), where we considered estimating a model for each season.

```{r echo=F}
seoul.elastic$residuals %>% 
  hist(main="Histogram of Residuals from Elastic Net Regression", col="steelblue")
```

Since the histogram of the residuals appears Normal, aside from the extreme values at the tails, we will appeal to Normality due to the large sample size.

Additionally, diagnostics ([A.2.1](#a21)) show that there no undully outliers, and linearity and homoskedasticity are met.

Thus, using our elastic regression results, we will modify Model (1) to include the following interaction terms. We made sure to include the lower-order regression coefficients for the coefficients that elastic net regression left out when elastic net included interaction terms involving a lower-order term. We are interested in estimating the most assailant factors ([A.5](#a5)). See [A.6](#a6) for the full model.

(2) $\Delta\text{Temperature}_{st} = ... + \beta_5\Delta\text{SolarRadiation}_{st} + ... + \beta_8\Delta I(\text{Hour}_{t} = 1) + ... + \beta_{30}\Delta I(\text{Hour}_{t} = 23) + ... + \beta_{32}\Delta\text{Humidity}_{st}\text{SolarRadiation}_{st} + ... + \beta_{39}\Delta\text{SolarRadiation}_{st}\text{Rainfall}_{st} + ... + \sum_{n=1}^{23}{\beta_{131+n}\Delta\text{SolarRadiation}_{st}\Delta I(\text{Hour}_t=n)} + ... + \Delta\eta_{st}$

$\Delta\eta_{st} \overset{iid}{\sim} N(0, \sigma^2)$

---

In order to estimate the hourly demand for bike sharing in Seoul. Similar to our model for temperature, we have unobserved seasonal effects in $\epsilon$. However, exploratory data analysis has shown that current bike demand is not strongly dependent on the bike demand from $t-1$. Instead of using a first-difference approach, we will use a fixed effects models to include the seasonal effects for $s-1$ (3) seasons to obtain unbiased estimates. We will use Spring as our base season.

```{r}
l = 1
alpha = 0.05
deltas = cov(seoul$RentedBikeCount[(l+1):length(seoul$RentedBikeCount)], 
    lag(seoul$RentedBikeCount, n=l)[(l+1):length(seoul$RentedBikeCount)])/
var(seoul$RentedBikeCount[(l+1):length(seoul$RentedBikeCount)])
p.value = 0

while(p.value < alpha){
  l = l+1
  
  deltas = c(deltas, cov(seoul$RentedBikeCount[(l+1):length(seoul$RentedBikeCount)], 
    lag(seoul$RentedBikeCount, n=l)[(l+1):length(seoul$RentedBikeCount)])/
  var(seoul$RentedBikeCount[(l+1):length(seoul$RentedBikeCount)]))
  
  l.lm = lm(seoul$RentedBikeCount[(l+1):length(seoul$RentedBikeCount)] ~ 
       lag(seoul$RentedBikeCount, n=l)[(l+1):length(seoul$RentedBikeCount)]) %>%
    summary()
  p.value = l.lm$coefficients[, "Pr(>|t|)"][2]
}

find_local_max = function(v){
  if(v[2] <= v[1]){
    maxes = v[1]
  } else {
    maxes = c()
  }
  
  for(i in 1:length(v)){
    if(i-1 >= 1 && i+1 <= length(v)){
      if(v[i-1] <= v[i] && 
         v[i+1] <= v[i]){
        maxes = c(maxes, v[i])
      }
    }
  }
  
  if(v[length(v)] >= v[(length(v)-1)]){
    maxes = c(maxes, v[length(v)])
  }
  
  return(maxes)
}

local.max = find_local_max(deltas)
deltas.max = which(deltas %in% local.max)

ggplot(mapping=aes(x=1:l, y=deltas))+
  geom_point()+
  geom_line()+
  xlab("Lags (Hours Ago)")+
  ylab(expression(delta))+
  scale_x_continuous(breaks=deltas.max)
```

We transformed $\text{RentedBikeCount}_{st}$ with a $ln$ transformation. Upon further analysis, we also transformed $\text{Visibility}_{st}$ with a $f(x)=-\sqrt{x}$ transformation. We also noticed that different levels of $\text{Humidity}_{st}$ were affecting $\text{RentedBikeCount}_{st}$ in a non-linear fashion. Hence, we created 10 different levels for humidity as dummy variables, omitting the 0-10% level as the base level. We also noticed that extreme levels of $\text{WindSpeed}_{st}$ were also affecting $\text{RentedBikeCount}_{st}$. We created another variable, $I(\text{WindSpeed}_{st} > \phi(\text{WindSpeed}_s,0.95))$

Hence, our basic fixed effects multivariate model for measuring the demand for rented bikes in Seoul will be interpreted as,

$ln(\text{RentedBikeCount}_{st}+1) = \beta_0 + \beta_1I(10 \leq Humidity_t < 20) + ... + \beta9I(90 \leq Humidity_t < 100) + \beta_{10}\text{Temperature}_{st} + \beta_{11}\text{WindSpeed}_{st} + \beta_{12}(-\sqrt{\text{Visibility}_{st}}) + \beta_{13}I(\text{ExtremeWind}_{st} = \text{Yes}) + \beta_{14}\text{DewPointTemp}_{st} + \beta_{15}\text{SolarRadiation}_{st} + \beta_{16}\text{Rainfall}_{st} + \beta_{17}\text{Snowfall}_{st} + \beta_{18}I(\text{Day}_t = Holiday) + \beta_{19}I(\text{Hour}_t = 1) + ... + \beta_{41}I(\text{Hour}_t = 23) + \beta_{42}I(\text{Season}_s = Winter) + \beta_{43}I(\text{Season}_s = Summer) + \beta_{44}I(\text{Season}_s = Autumn) + \eta_{st}$

## APPENDIX

#### A.0 {#a0}

Selection Process for Optimal Choice of $k$

We want $k$ as small as possible because we want as much data for our model: Our belief is that short-run fluctuations in temperature can be determined by other short-run changes in other factors. Hence, the smaller $k$ is, the better predictions we can make about the hour $t$. If we wanted to parse out a long-run effect, we would aggregate on a time period and regress over that interval. However, for the purposes of our research question, we need to be informed the most about what is happening in the short-run.

Running a $k$th difference regression model where $\Delta\text{Temperature}_t = \text{Temperature}_t - \text{Temperature}_{t-k}$ we can find a sufficient $k$ by regressing $\Delta\text{Temperature}_{t-1}$ on $\Delta\text{Temperature}_{t-k^*}$, where $k*$ is the proposed value of $k$. We will choose $k=k^*$ when the regression coefficient becomes non-significant. This (non-coincidentally) happens as the regression coefficient approaches 0, or as $\Delta\text{Temperature}_t$ becomes less and less dependent of the $\Delta\text{Temperature}$ from $k^*$ hours ago.

The optimal choice of $k$ will lead us to run a $k$th difference regression model where $\Delta\text{Temperature}_t = \text{Temperature}_t - \text{Temperature}_{t-k}$.

Optimal choice of $k$:

```{r}
ks = c()
alpha = 0.1
for(k in 1:100){
  k.change = seoul$Temperature - lag(seoul$Temperature, n = 1)
  k.lm = lm(k.change ~ lag(k.change, n = k))
  k.p = summary(k.lm)$coefficients[, "Pr(>|t|)"][2]
  if(k.p > alpha){
    ks = c(ks, k)
  }
}

ks
```

Hence, we chose $k$ = `r first(ks)`

#### A.1.0 {#a10}

Regression output and diagnostics for Model (1)

```{r}
seoul.lm = lm(TemperatureChange ~ 0 + ., data=seoul.temp[c(seoul.change)])
summary(seoul.lm)

autoplot(seoul.lm)
show_leverage(seoul.lm)
```

#### A.1.1 {#a11}

Added Variable Plots for Model (1). After considering the influential points, we determined that the linearity assumption was met.

```{r}
avPlots(seoul.lm)
```

#### A.2.0 {#a20}

Elastic Net Regression on Model (1) w/ interaction terms

```{r}
summary(seoul.elastic)
```

#### A.2.1 {#a21}

Model diagnostics for elastic net regression on Model (1) w/ interaction terms

Model assumptions did not drastically improve. QQ-plot indicates that normality assumption is still violated, and is confirmed by a Shapiro-Wilk normality test.

```{r}
autoplot(seoul.elastic)
shapiro.test(sample(seoul.elastic$residuals, 5000))

show_leverage(seoul.elastic)
```

#### A.3 {#a3}

Transformations we considered for Model (1)

```{r}
factors.transform = c("HumidityChange", "DewPointTempChange", 
                      "SolarRadiationChange", "WindSpeedChange")

t = seoul.temp %>% mutate(
  HumidityChange = HumidityChange + abs(min(HumidityChange)) + 1,
  DewPointTempChange = DewPointTempChange + abs(min(DewPointTempChange)) + 1,
  SolarRadiationChange = SolarRadiationChange + abs(min(SolarRadiationChange)) + 1,
  WindSpeedChange = WindSpeedChange + abs(min(WindSpeedChange)) + 1
)

for(i in 1:length(factors.transform)){
  factor = factors.transform[i]
  invTranPlot(formula(str_c("TemperatureChange ~ ", factor)), data=t, 
            lambda = c(-1, -0.5, 0, 0.5, 1, 2), optimal=T)
}
```

#### A.4 {#a4}

An alternative solution to meeting the assumptions for Model (1). In theory,

$\text{Var}(\eta_{st}|s) = \sigma^2 \quad \forall s \in \text{Seasons}$

However, if it's the case that

$\exists \text{ at least one } s \text{ s.t. } \text{Var}(\eta_{st}|s) \neq \sigma^2$

Then, we could use 4 separate models for each season to completely eliminate $s$ out of the general $\eta_{st}$. Hence, each season will have its own population variance for $\eta_t$.

```{r}
create_season_models = function(factors, response, base=F){
  factors = setdiff(factors, response)
  if(base == T){
    f = 0
  } else {
    f = paste(c(0, factors), collapse = " + ")
  }

  models <- map(unique(seoul$Seasons), function(x){
    lm(formula(paste(response, "~", f)),
       data = seoul[seoul$Seasons == x,] %>%
         select(all_of(c(factors, response))))
  })

  names(models) <- str_c("seoul.lm.", unique(seoul$Seasons))
  return(models)
}

models.lm = create_season_models(seoul.change, "TemperatureChange")

create_elastic = function(){
  models = list()
  seasons = seoul$Seasons %>% unique()
  for(s in seasons){
    #print(s)
    seoul.subset = seoul.temp %>% filter(Seasons == s)
    seoul.x = as.matrix(seoul.subset[model.factors])
    seoul.y = as.matrix(seoul.subset["TemperatureChange"])

    elastic_cv <- cv.glmnet(x = seoul.x,
                          y = seoul.y,
                          type.measure = "mse",
                          alpha = 0.5)

    d = coef(elastic_cv, s = "lambda.1se")[2:(length(model.factors)-1)]
    elastic.factors = model.factors[which(d != 0)]
    models[[str_c("seoul.lasso.",s)]] = lm(TemperatureChange ~ 0 + .,
                                           seoul.subset[c("TemperatureChange",
                                           elastic.factors)])
  }
  return(models)
}
seoul.elastic.models = create_elastic()
for(model in seoul.elastic.models){
  print(summary(model))
  print(autoplot(model))
}
```

#### A.5 {#a5}

Most assailant factors in Model (2) (hour changes excluded)

```{r}
elastic.coef = tibble(
  Factor = names(seoul.elastic$coefficients),
  Coefficient = seoul.elastic$coefficients
)

elastic.coef %>% arrange(abs(Coefficient) %>% desc()) %>% 
  filter(!(Factor %in% str_c(time.bike, "Change"))) %>%
  knitr::kable()
```

#### A.6 {#a6}

The theoretical model for Model (2) can be fully written as,

(2) $\Delta\text{Temperature}_{st} = \beta_1\Delta\text{Humidity}_{st} + \beta_2\Delta\text{DewPointTemp}_{st} + \beta_3\Delta\text{Visibility}_{st} + \beta_4\Delta\text{WindSpeed}_{st} + \beta_5\Delta\text{SolarRadiation}_{st} + \beta_6\Delta\text{Rainfall}_{st} + \beta_7\Delta\text{Snowfall}_{st} + \beta_8\Delta I(\text{Hour}_{t} = 1) + ... + \beta_{30}\Delta I(\text{Hour}_{t} = 23) + \beta_{31}\Delta\text{Humidity}_{st}\text{DewPointTemp}_{st} + \beta_{32}\Delta\text{Humidity}_{st}\text{SolarRadiation}_{st}+\beta_{33}\Delta\text{Humidity}_{st}\text{Rainfall}_{st} + \beta_{34}\Delta\text{Humidity}_{st}\text{Snowfall}_{st} + \beta_{35}\Delta\text{DewPointTemp}_{st}\text{WindSpeed}_{st} + \beta_{36}\Delta\text{DewPointTemp}_{st}\text{Rainfall}_{st} + \beta_{37}\Delta\text{WindSpeed}_{st}\text{Visibility}_{st}+\beta_{38}\Delta\text{WindSpeed}_{st}\text{solarRadiation}_{st} + \beta_{39}\Delta\text{SolarRadiation}_{st}\text{Rainfall}_{st}+ \\ \sum_{j=1}^{23}{\beta_{39+j}\Delta\text{Humidity}_{st}\Delta I(\text{Hour}_t=j)}+\sum_{k=1}^{23}{\beta_{62+k}\Delta\text{DewPointTemp}_{st}\Delta I(\text{Hour}_t=k)}+\sum_{l=1}^{23}{\beta_{85+l}\Delta\text{WindSpeed}_{st}\Delta I(\text{Hour}_t=l)}+\sum_{m=1}^{23}{\beta_{108+m}\Delta\text{Visibility}_{st}\Delta I(\text{Hour}_t=m)}+\sum_{n=1}^{23}{\beta_{131+n}\Delta\text{SolarRadiation}_{st}\Delta I(\text{Hour}_t=n)}+\sum_{o=1}^{23}{\beta_{154+o}\Delta\text{Rainfall}_{st}\Delta I(\text{Hour}_t=o)}+\sum_{p=1}^{23}{\beta_{177+p}\Delta\text{Snowfall}_{st}\Delta I(\text{Hour}_t=p)} + \Delta\eta_{st}$

$\Delta\eta_{st} \overset{iid}{\sim} N(0, \sigma^2)$

Due to the nature of elastic net regression, some of the coefficients for $j, k, l, m, n, o,$ and $p$ are $0$:

$\beta_{39+j} = 0 \quad \forall j \in \{3,13,15,18,20\}$

$\beta_{62+k} = 0 \quad \forall k \in \{3,5,6,7,8,12,15,22,23\}$

$\beta_{85+l} = 0 \quad \forall l \in \{1,2,3,7,8,9,12,13,14,16,20,21,22,23\}$

$\beta_{108+m} = 0 \quad \forall m \in \{5,6,7,,10,12,13,14,15,18,21,22\}$

$\beta_{131+n} = 0 \quad \forall n \in \{8,9,15,16,18,21,22\}$

$\beta_{154+o} = 0 \quad \forall o \in \{1,...,8,11,12,13,15,16,17,18,19,23\}$

$\beta_{177+p} = 0 \quad \forall p \in \{1,...,7,11,...,15,17,18,20,21,22,23\}$